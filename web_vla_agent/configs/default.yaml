# ============================================================
# VLA Web Agent â€” Configuration (Multimodal Sequential VLM)
# ============================================================

model:
  # Multimodal VLM backbone
  name: "Qwen/Qwen2-VL-2B-Instruct"
  max_new_tokens: 256
  temperature: 0.1
  top_p: 0.9
  repetition_penalty: 1.1

  # QLoRA configuration
  use_qlora: true
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  lora_target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  quantization_bits: 4

  # Action types
  action_types: ["CLICK", "TYPE", "SELECT", "SCROLL"]

training:
  # Stage 1: Single-step imitation
  stage1_epochs: 5
  # Stage 2: Multi-step imitation
  stage2_epochs: 10
  # Stage 3 (optional): RL fine-tuning
  stage3_epochs: 0

  learning_rate: 2.0e-4
  weight_decay: 0.01
  batch_size: 4
  gradient_accumulation_steps: 4
  warmup_ratio: 0.1
  max_grad_norm: 1.0
  seed: 42

  # Mixed precision
  fp16: false
  bf16: true

  # Checkpointing
  save_every_n_epochs: 1
  checkpoint_dir: "checkpoints"
  logging_steps: 10

  # Dataset
  max_seq_length: 4096

data:
  dataset_name: "osunlp/Multimodal-Mind2Web"
  max_dom_nodes: 500
  max_action_history: 10
  max_text_per_node: 200

environment:
  headless: true
  viewport_width: 1280
  viewport_height: 720
  timeout_ms: 30000
  max_steps: 30

uncertainty:
  # Token-level confidence
  min_log_prob_threshold: -2.0   # average log-prob below this triggers regeneration
  beam_width: 3                   # for beam disagreement
  max_regenerations: 2            # max re-generation attempts

evaluation:
  splits: ["test_task", "test_website", "test_domain"]

logging:
  level: "INFO"
  log_dir: "logs"
  use_wandb: false
  wandb_project: "web-vla-agent"
